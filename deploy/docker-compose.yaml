version: '3'

services:
  hub-ui:
    image: hub-ui:1.0.3
    container_name: hub-ui
    ports:
      - 81:81
      - 82:82
      - 83:83
      - 84:84
      - 85:85
      - 86:86
    volumes:
      - ./hub-ui:/var/log/nginx
      - /etc/localtime:/etc/localtime
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:81"]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      hub-gateway:
        condition: service_healthy
    networks:
      default:
        ipv4_address: 10.64.32.10

  hub-gateway:
    image: hub-gateway:1.0.3
    container_name: hub-gateway
    ports:
      - 19000:19000
    environment:
      TZ: Asia/Shanghai
      jvm_option: -Xms256m -Xmx256m -XX:MetaspaceSize=128m -XX:+HeapDumpOnOutOfMemoryError
      http_port: 19000
      nacos_server: hub-nacos:8848
      nacos_namespace:
      nacos_cluster:
      nacos_group: DEFAULT_GROUP
      redis_host: hub-redis
      redis_port: 6379
    volumes:
      - ./hub-gateway:/opt/cowave/hub-gateway/log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19000/actuator/health"]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      hub-nacos:
        condition: service_healthy
    networks:
      default:
        ipv4_address: 10.64.32.12

  hub-blog:
    image: hub-blog:1.0.3
    container_name: hub-blog
    ports:
      - 80:80
    environment:
      TZ: Asia/Shanghai
      jvm_option: -Xms256m -Xmx256m -XX:MetaspaceSize=128m -XX:+HeapDumpOnOutOfMemoryError
      app_profile: prod
      http_port: 80
      nacos_server: hub-nacos:8848
      nacos_namespace:
      nacos_cluster:
      nacos_group: DEFAULT_GROUP
      redis_host: hub-redis
      redis_port: 6379
      datasource_username: postgres
      datasource_password: postgres
      datasource_url: jdbc:postgresql://hub-postgres:5432/hub-blog
      hub_admin_uri: http://10.64.4.74:81
      oauth_app_id: 6ac6519451ed4ef09431aacccbcb1f5f
      oauth_app_secret: 4a2e671fbd074f238e80c7f5566f8f7a
      oauth_app_redirect: http://10.64.4.74/auth/callback
      oauth_code_uri: http://10.64.4.74:81/oauth/cowave/authorize
      oauth_token_uri: http://10.64.4.74:19000
    volumes:
      - ./hub-blog/log:/opt/cowave/hub-blog/log
      - ./hub-blog/public:/opt/cowave/hub-blog/public
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/actuator/health"]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      hub-nacos:
        condition: service_healthy
      hub-postgres:
        condition: service_healthy
    networks:
      default:
        ipv4_address: 10.64.32.13

  hub-admin:
    image: hub-admin:1.0.3
    container_name: hub-admin
    ports:
      - 19010:19010
      - 19011:19011
      - 19012:19012
    environment:
      TZ: Asia/Shanghai
      jvm_option: -Xms256m -Xmx256m -XX:MetaspaceSize=128m -XX:+HeapDumpOnOutOfMemoryError -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:19012
      app_profile: prod
      http_port: 19010
      socket_port: 19011
      nacos_server: hub-nacos:8848
      nacos_namespace:
      nacos_cluster:
      nacos_group: DEFAULT_GROUP
      elastic_url: hub-elastic:9200
      kafka_servers: hub-kafka1:9092,hub-kafka2:9093,hub-kafka3:9094
      redis_host: hub-redis
      redis_port: 6379
      datasource_username: postgres
      datasource_password: postgres
      datasource_url: jdbc:postgresql://hub-postgres:5432/hub-admin
      minio_endpoint: http://10.64.4.74:39000
      minio_accessKey: admin
      minio_secretKey: admin123
    volumes:
      - ./hub-admin:/opt/cowave/hub-admin/log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19010/admin/actuator/health"]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      hub-postgres:
        condition: service_healthy
      hub-redis:
        condition: service_healthy
      hub-elastic:
        condition: service_healthy
      hub-nacos:
        condition: service_healthy
    networks:
      default:
        ipv4_address: 10.64.32.14

  hub-flow:
    image: hub-flow:1.0.3
    container_name: hub-flow
    ports:
      - 19020:19020
      - 19021:19021
    environment:
      TZ: Asia/Shanghai
      jvm_option: -Xms256m -Xmx256m -XX:MetaspaceSize=128m -XX:+HeapDumpOnOutOfMemoryError -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:19021
      app_profile: prod
      http_port: 19020
      nacos_server: hub-nacos:8848
      nacos_namespace:
      nacos_cluster:
      nacos_group: DEFAULT_GROUP
      datasource_username: postgres
      datasource_password: postgres
      datasource_url: jdbc:postgresql://hub-postgres:5432/hub-admin
    volumes:
      - ./hub-flow:/opt/cowave/hub-flow/log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19020/flow/actuator/health"]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      hub-postgres:
        condition: service_healthy
      hub-nacos:
        condition: service_healthy
      hub-admin:
        condition: service_healthy
    networks:
      default:
        ipv4_address: 10.64.32.15

  hub-meter:
    image: hub-meter:1.0.3
    container_name: hub-meter
    ports:
      - 19030:19030
      - 19031:19031
    environment:
      TZ: Asia/Shanghai
      jvm_option: -Xms256m -Xmx256m -XX:MetaspaceSize=128m -XX:+HeapDumpOnOutOfMemoryError -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:19031
      app_profile: prod
      http_port: 19030
      nacos_server: hub-nacos:8848
      nacos_namespace:
      nacos_cluster:
      nacos_group: DEFAULT_GROUP
      datasource_username: postgres
      datasource_password: postgres
      datasource_url: jdbc:postgresql://hub-postgres:5432/hub-meter
    volumes:
      - ./hub-meter:/opt/cowave/hub-meter/log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19030/meter/actuator/health"]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      hub-postgres:
        condition: service_healthy
      hub-nacos:
        condition: service_healthy
    networks:
      default:
        ipv4_address: 10.64.32.16

  hub-job:
    image: hub-job:1.0.3
    container_name: hub-job
    ports:
      - 19040:19040
      - 19041:19041
    environment:
      TZ: Asia/Shanghai
      jvm_option: -Xms256m -Xmx256m -XX:MetaspaceSize=128m -XX:+HeapDumpOnOutOfMemoryError -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:19041
      app_profile: prod
      http_port: 19040
      nacos_server: hub-nacos:8848
      nacos_namespace:
      nacos_cluster:
      nacos_group: DEFAULT_GROUP
      datasource_username: postgres
      datasource_password: postgres
      datasource_url: jdbc:postgresql://hub-postgres:5432/hub-job
    volumes:
      - ./hub-job:/opt/cowave/hub-job/log
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19040/job/actuator/health"]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      hub-postgres:
        condition: service_healthy
      hub-nacos:
        condition: service_healthy
    networks:
      default:
        ipv4_address: 10.64.32.17

  hub-nacos:
    image: hub-nacos:2.3.0
    container_name: hub-nacos
    environment:
      - MODE=standalone
      - cluster_ip=10.64.4.74
      - DB_POOL_CONFIG_CONNECTIONTIMEOUT=6000000
      - datasource_username=postgres
      - datasource_password=postgres
      - datasource_url=jdbc:postgresql://hub-postgres:5432/nacos?tcpKeepAlive=true&reWriteBatchedInserts=true
    volumes:
      - ./hub-nacos:/home/nacos/logs
    ports:
      - "8848:8848"
      - "9848:9848"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8848/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 20
    networks:
      default:
        ipv4_address: 10.64.32.11

  hub-postgres:
    image: postgres:13.1
    container_name: hub-postgres
    ports:
      - 5433:5432
    environment:
      TZ: Asia/Shanghai
      POSTGRES_PASSWORD: postgres
    volumes:
      - ./postgres:/var/lib/postgresql/data
      - /etc/localtime:/etc/localtime
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      default:
        ipv4_address: 10.64.32.21

  hub-mysql:
    image: mysql:8.0.36
    container_name: hub-mysql
    privileged: true
    ports:
      - 3307:3306
    environment:
      TZ: Asia/Shanghai
      MYSQL_ROOT_PASSWORD: root
    volumes:
      - ./mysql/conf:/etc/mysql/conf.d
      - ./mysql/log:/var/log
      - ./mysql/data:/var/lib/mysql
      - /etc/localtime:/etc/localtime
    healthcheck:
      test: ["CMD", "mysqladmin", "-uroot", "-proot", "ping"]
      interval: 5s
      timeout: 5s
      retries: 10
    networks:
      default:
        ipv4_address: 10.64.32.22

  hub-elastic:
    image: elastic/elasticsearch:7.17.3
    container_name: hub-elastic
    ports:
      - 9200:9200
    environment:
      - ES_JAVA_OPTS=-Xms2048m -Xmx2048m
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - ./elastic/plugins:/usr/share/elasticsearch/plugins
      - ./elastic/data:/usr/share/elasticsearch/data
      - ./elastic/log:/usr/share/elasticsearch/log
      - /etc/localtime:/etc/localtime
    healthcheck:
      test: [ "CMD", "curl", "-fs", "http://localhost:9200/_cluster/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      default:
        ipv4_address: 10.64.32.29

  hub-redis:
    image: redis:7.0
    container_name: hub-redis
    privileged: true
    ports:
      - 6379:6379
    volumes:
      - ./redis/conf/redis.conf:/etc/redis/redis.conf
      - ./redis/data:/data:rw
      - /etc/localtime:/etc/localtime
    command: redis-server /etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      default:
        ipv4_address: 10.64.32.23

  hub-minio:
    image: minio/minio
    container_name: hub-minio
    privileged: true
    ports:
      - 39000:9000
      - 39001:9001
    environment:
      MINIO_ACCESS_KEY: admin
      MINIO_SECRET_KEY: admin123
    volumes:
      - ./minio/data:/data
      - ./minio/config:/root/.minio/
      - /etc/localtime:/etc/localtime
    command: server --console-address ':9001' /data
    healthcheck:
      test: [ "CMD", "bash", "-c", "< /dev/tcp/localhost/9000" ]
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      default:
        ipv4_address: 10.64.32.24

  hub-kafka1:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: hub-kafka1
    ports:
      - "9092:9092"
      - "9097:9097"
      - "7072:7072"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://10.64.4.74:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: hub-zookeeper:2181
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE: true
      KAFKA_LOG_RETENTION_HOURS: 48
      JMX_PORT: 9097
      KAFKA_OPTS: -javaagent:/opt/jmx_prometheus_javaagent-0.12.0.jar=7072:/opt/kafka-2_0_0.yml
    volumes:
      - /etc/localtime:/etc/localtime
      - ./kafka/jmx_prometheus_javaagent-0.12.0.jar:/opt/jmx_prometheus_javaagent-0.12.0.jar
      - ./kafka/kafka-2_0_0.yml:/opt/kafka-2_0_0.yml
    healthcheck:
      test: [ "CMD", "bash", "-c", "< /dev/tcp/localhost/9092" ]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      - hub-zookeeper
    networks:
      default:
        ipv4_address: 10.64.32.25

  hub-kafka2:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: hub-kafka2
    ports:
      - "9093:9093"
      - "9098:9098"
      - "7073:7073"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://10.64.4.74:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093
      KAFKA_ZOOKEEPER_CONNECT: hub-zookeeper:2181
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE: true
      KAFKA_LOG_RETENTION_HOURS: 48
      JMX_PORT: 9098
      KAFKA_OPTS: -javaagent:/opt/jmx_prometheus_javaagent-0.12.0.jar=7073:/opt/kafka-2_0_0.yml
    volumes:
      - /etc/localtime:/etc/localtime
      - ./kafka/jmx_prometheus_javaagent-0.12.0.jar:/opt/jmx_prometheus_javaagent-0.12.0.jar
      - ./kafka/kafka-2_0_0.yml:/opt/kafka-2_0_0.yml
    healthcheck:
      test: [ "CMD", "bash", "-c", "< /dev/tcp/localhost/9093" ]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      - hub-zookeeper
    networks:
      default:
        ipv4_address: 10.64.32.26

  hub-kafka3:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: hub-kafka3
    ports:
      - "9094:9094"
      - "9099:9099"
      - "7074:7074"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://10.64.4.74:9094
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094
      KAFKA_ZOOKEEPER_CONNECT: hub-zookeeper:2181
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE: true
      KAFKA_LOG_RETENTION_HOURS: 48
      JMX_PORT: 9099
      KAFKA_OPTS: -javaagent:/opt/jmx_prometheus_javaagent-0.12.0.jar=7074:/opt/kafka-2_0_0.yml
    volumes:
      - /etc/localtime:/etc/localtime
      - ./kafka/jmx_prometheus_javaagent-0.12.0.jar:/opt/jmx_prometheus_javaagent-0.12.0.jar
      - ./kafka/kafka-2_0_0.yml:/opt/kafka-2_0_0.yml
    healthcheck:
      test: [ "CMD", "bash", "-c", "< /dev/tcp/localhost/9094" ]
      interval: 5s
      timeout: 3s
      retries: 10
    depends_on:
      - hub-zookeeper
    networks:
      default:
        ipv4_address: 10.64.32.27

  hub-zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    container_name: hub-zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - /etc/localtime:/etc/localtime
    healthcheck:
      test: [ "CMD", "bash", "-c", "< /dev/tcp/localhost/2181" ]
      interval: 5s
      timeout: 3s
      retries: 10
    networks:
      default:
        ipv4_address: 10.64.32.28

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - 3000:3000
    environment:
      - TZ=Asia/Shanghai
    volumes:
      - ./grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./grafana/linux.json:/usr/share/grafana/public/dashboards/linux.json
    networks:
      default:
        ipv4_address: 10.64.32.51

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/rules:/etc/prometheus/rules
      - /etc/hosts:/etc/hosts
    ports:
      - "9090:9090"
    environment:
      - TZ=Asia/Shanghai
    networks:
      default:
        ipv4_address: 10.64.32.52

  alertmanager:
    image: bitnami/alertmanager:0.26.0
    container_name: alertmanager
    ports:
      - "9083:9093"
    volumes:
      - ./alertmanager:/etc/alertmanager
      - /etc/localtime:/etc/localtime
    command:
      - '--cluster.advertise-address=0.0.0.0:9093'
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    networks:
      default:
        ipv4_address: 10.64.32.53

  loki:
    image: grafana/loki
    container_name: loki
    privileged: true
    user: root
    ports:
      - "3100:3100"
    volumes:
      - ./loki/loki-config.yaml:/etc/loki/loki-config.yaml
      - ./loki/rules:/opt/app/loki/rules
    command: -config.file=/etc/loki/loki-config.yaml
    networks:
      default:
        ipv4_address: 10.64.32.54

  promtail:
    image: grafana/promtail
    container_name: promtail
    volumes:
      - ./promtail/promtail-config.yaml:/etc/promtail/promtail-config.yaml.yml
      - .:/opt/hub
    command: -config.file=/etc/promtail/promtail-config.yaml.yml
    networks:
      default:
        ipv4_address: 10.64.32.55

  node-exporter:
    image: prom/node-exporter
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - "/proc:/host/proc:ro"
      - "/sys:/host/sys:ro"
      - "/:/rootfs:ro"
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
    networks:
      default:
        ipv4_address: 10.64.32.61

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis://hub-redis:6379
    networks:
      default:
        ipv4_address: 10.64.32.62

  mysql-exporter:
    image: prom/mysqld-exporter
    container_name: mysql-exporter
    ports:
      - "9104:9104"
    volumes:
      - ./mysql/my.cnf:/etc/mysql/my.cnf
    command: --config.my-cnf=/etc/mysql/my.cnf
    networks:
      default:
        ipv4_address: 10.64.32.63

networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 10.64.32.0/24
          gateway: 10.64.32.1
